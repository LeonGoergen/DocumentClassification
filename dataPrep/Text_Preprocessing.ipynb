{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lAnpdTNgmM_9",
        "7EkLwCW8mtNC",
        "_qD3x0uJU6OJ",
        "3XsPiZWLVCSZ"
      ],
      "authorship_tag": "ABX9TyM+wezjP5NjyPN/N5wrzmV2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonGoergen/DocumentClassification/blob/main/dataPrep/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "lAnpdTNgmM_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNrKEnT55SSd",
        "outputId": "cd0478db-110e-46c0-e1b7-3a20b14c200d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import nltk\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Vb18Hs0Dmvq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f7a822-861c-4c4d-d6b0-64076aaeba61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "569sLhRtroo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02462fd0-51b6-499a-c571-04c5f0313013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "7EkLwCW8mtNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook preprocesses the textual content of the created datasets from the [dataset preparation notebook](https://colab.research.google.com/drive/10vfHyaNdtTaYq7TEIpW1b_kTgN2coIip?usp=sharing)"
      ],
      "metadata": {
        "id": "8D9PK4Jmix3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Bachelor Thesis/Datasets/train_nopreprocess.csv', sep=\"\\t\", header=0)\n",
        "test = pd.read_csv('/content/drive/MyDrive/Bachelor Thesis/Datasets/test_nopreprocess.csv', sep=\"\\t\", header=0)"
      ],
      "metadata": {
        "id": "elN4Z8ijmMsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "total_tokens = 0\n",
        "total_chars = 0\n",
        "all_tokens = []\n",
        "\n",
        "all_df = train.append(test)\n",
        "# iterate through each document in the \"Consumer complaint narrative\" column\n",
        "for i, row in tqdm(all_df.iterrows(), total=all_df.shape[0]):\n",
        "    # tokenize the document\n",
        "    tokens = tokenizer.encode(row[\"Consumer complaint narrative\"], max_length=2048, truncation=True)\n",
        "    # add tokens to list\n",
        "    all_tokens.extend(tokens)\n",
        "    # add the number of tokens to the total\n",
        "    total_tokens += len(tokens)\n",
        "    # add the number of characters to the total\n",
        "    total_chars += len(row[\"Consumer complaint narrative\"])\n",
        "\n",
        "# calculate the average number of tokens and characters per document\n",
        "avg_tokens = total_tokens / len(all_df)\n",
        "avg_chars = total_chars / len(all_df)\n",
        "\n",
        "print(\"\\nAverage tokens per document:\", avg_tokens)\n",
        "print(\"Average characters per document:\", avg_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x125mhV-nVgw",
        "outputId": "6318f8c0-7280-4de6-fd96-9dba2990a4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94260/94260 [03:01<00:00, 518.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average tokens per document: 276.6344260555909\n",
            "Average characters per document: 1224.7463505198386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tokens = len(set(all_tokens))\n",
        "print(\"Number of unique tokens in dataset:\", unique_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDjeRd2bpaba",
        "outputId": "11053fd4-8d49-401c-9cc0-66e8d8ad5ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens in dataset: 34016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define a function to count the number of x-character blocks in a document\n",
        "def count_blocks(document, x):\n",
        "    return math.ceil(len(document) / x)\n",
        "\n",
        "# Apply the function to the text column of the DataFrame and sum the result\n",
        "block_size = 100\n",
        "num_blocks = all_df['Consumer complaint narrative'].apply(count_blocks, args=(block_size,)).sum()\n",
        "print(f\"Ratio for 100-character-blocks: {num_blocks/all_df.shape[0]}\")\n",
        "\n",
        "block_size = 1000\n",
        "num_blocks = all_df['Consumer complaint narrative'].apply(count_blocks, args=(block_size,)).sum()\n",
        "print(f\"Ratio for 1000-character-blocks: {num_blocks/all_df.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swdfh2aAyR2a",
        "outputId": "ffc000a5-3f28-4670-b867-15b3518d9588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratio for 100-character-blocks: 12.741438574156588\n",
            "Ratio for 1000-character-blocks: 1.7471461913855293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['Consumer complaint narrative'].values[18]"
      ],
      "metadata": {
        "id": "XfM0sFkUhQ5Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1ef72bc2-50e9-4cdb-9642-570aebf55d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I initially obtained a loan in XX/XX/XXXX for a car lease for a XXXX XXXX XXXX, I obtained a loan from XXXX XXXX XXXX XXXX XXXX, as of XXXX it was changed to XXXX. I am in Predatory loan and need help out ; I am XXXX upside down in this loan from XXXX they were going to report me to the credit agency for {$27.00} yes XXXX dollars. When you ask for extension, they put so much interest on the deferred payment plan. I don't think I will ever be done paying for this loan, Please Help.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['Product'].values[18]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-y24s8GTNmkO",
        "outputId": "f2095884-7ad2-49d2-aafe-1497bd683783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Vehicle loan or lease'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['Complaint ID'].values[18]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7zTofaGixec",
        "outputId": "0c78c7ff-c4b9-488f-9631-bd01ec64019e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5937822"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n",
        "    punctuationfree = re.sub(r'[\\W\\n\\t]+', ' ', punctuationfree)\n",
        "    return punctuationfree\n",
        "\n",
        "#storing the punctuation free text\n",
        "train['Consumer complaint narrative'] = train['Consumer complaint narrative'].apply(lambda x:remove_punctuation(x))\n",
        "test['Consumer complaint narrative'] = test['Consumer complaint narrative'].apply(lambda x:remove_punctuation(x))\n",
        "train['Consumer complaint narrative'].values[18]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "TIt-iO8EA2aL",
        "outputId": "8d1a0007-0720-4f5c-a9ce-6f9259eecb49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I initially obtained a loan in XXXXXXXX for a car lease for a XXXX XXXX XXXX I obtained a loan from XXXX XXXX XXXX XXXX XXXX as of XXXX it was changed to XXXX I am in Predatory loan and need help out I am XXXX upside down in this loan from XXXX they were going to report me to the credit agency for 2700 yes XXXX dollars When you ask for extension they put so much interest on the deferred payment plan I dont think I will ever be done paying for this loan Please Help'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Datasets for commercial providers"
      ],
      "metadata": {
        "id": "_qD3x0uJU6OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Commercial Providers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "train_basic = train[['Consumer complaint narrative', 'Product']]\n",
        "test_basic = test[['Consumer complaint narrative', 'Product']]\n",
        "train_basic = train_basic.rename(columns={'Consumer complaint narrative': 'text', 'Product': 'label'})\n",
        "test_basic = test_basic.rename(columns={'Consumer complaint narrative': 'text', 'Product': 'label'})\n",
        "\n",
        "train_basic.to_csv(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/train_basic.csv\", sep=',', index=False) # Levity, PlanetAI\n",
        "test_basic.to_csv(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/test_basic.csv\", sep=',', index=False) # For All\n",
        "\n",
        "train_split_basic, val_split_basic = train_test_split(train_basic, test_size=0.15)\n",
        "\n",
        "train_split_basic.to_csv(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/train_split_basic.csv\", sep=',', index=False) # Google\n",
        "val_split_basic.to_csv(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/val_split_basic.csv\", sep=',', index=False) # Google\n",
        "\n",
        "train_basic_aws = train[['Product', 'Consumer complaint narrative']]\n",
        "train_basic_aws.to_csv(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/train_basic_aws.csv\", sep=\",\", quoting=csv.QUOTE_ALL, index=False) # for AWS"
      ],
      "metadata": {
        "id": "J1Rd1mEHHOTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI\n",
        "\n",
        "import json\n",
        "\n",
        "# Get the unique values in the label column and sort them\n",
        "label_values = sorted(train['Product'].unique())\n",
        "\n",
        "# Map the label values to numerical labels\n",
        "label_map = {value: index+1 for index, value in enumerate(label_values)}\n",
        "\n",
        "# Loop over each row in your DataFrame and generate a JSON object for each\n",
        "json_list = []\n",
        "for index, row in train.iterrows():\n",
        "    # Get the label and text for this row\n",
        "    label = row['Product']\n",
        "    text = row['Consumer complaint narrative']\n",
        "\n",
        "    # Map the label to a numerical value\n",
        "    label_num = label_map[label]\n",
        "\n",
        "    # Construct the prompt string with the text and ### separator\n",
        "    prompt = f\"Content:{text}\\n\\n###\\n\\n\"\n",
        "\n",
        "    # Construct the completion string with the label number\n",
        "    completion = f\" {label_num}\"\n",
        "\n",
        "    # Construct the JSON object with prompt and completion\n",
        "    json_obj = {'prompt': prompt, 'completion': completion}\n",
        "\n",
        "    # Add the JSON object to the list\n",
        "    json_list.append(json_obj)\n",
        "\n",
        "print(label_map)\n",
        "print('')\n",
        "json_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7cLPSBaqJTj",
        "outputId": "98cf40db-221d-4a5a-ad16-41d5d5a1d7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Bank, checking or savings account': 1, 'Consumer Loan': 2, 'Credit card or prepaid card': 3, 'Credit reporting, credit repair services, or other personal consumer reports': 4, 'Debt collection': 5, 'Money transfer, virtual currency, or money service': 6, 'Mortgage': 7, 'Payday loan, title loan, or personal loan': 8, 'Student loan': 9, 'Vehicle loan or lease': 10}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': 'Content:Date XXXX Amount XXXX litecoin Company XXXX Coinbase inc My account XXXX Action Failing to recognize an erroneous transaction I was trying to transfer Litecoin to my another account However I accidentally entered my Bitcoin address instead of my Litecoin address as the receiving address Of course this got rejected by the blockchain right away However XXXX confirmed this as a valid transaction As a consequence the XXXX litecoin has been withdrawn from my GDAX account although the transaction never happened Ever since Litecoin had dropped 30 in value and I lost the opportunity to sell it since XXXX had effectively confiscated my litecoins I contacted their customer supports and they havent got back to me in 2 months I attached two screenshots One shows the transfer history from my XXXX account and it says the transfer is complete However if I track the transfer address it says the Litecoins were never redeemed ie the transfer was never completed \\n\\n###\\n\\n',\n",
              " 'completion': ' 6'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/json_file_train_openai\", \"w\") as fp:\n",
        "    json.dump(json_list, fp)"
      ],
      "metadata": {
        "id": "I8SIrC42spET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Microsoft Azure\n",
        "\n",
        "json_file = {\n",
        "    \"projectFileVersion\": \"2022-05-01\",\n",
        "    \"stringIndexType\": \"Utf16CodeUnit\",\n",
        "    \"metadata\": {\n",
        "      \"projectKind\": \"CustomSingleLabelClassification\",\n",
        "      \"storageInputContainerName\": \"storagecontainer\",\n",
        "      \"settings\": {},\n",
        "      \"projectName\": \"document_classification_project\",\n",
        "      \"multilingual\": False,\n",
        "      \"description\": \"\",\n",
        "      \"language\": \"en-us\"\n",
        "    },\n",
        "    \"assets\": {\n",
        "      \"projectKind\": \"CustomSingleLabelClassification\",\n",
        "      \"classes\": [\n",
        "          {\n",
        "              \"category\": \"Payday loan, title loan, or personal loan\"\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Money transfer, virtual currency, or money service\"\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Mortgage\"\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Bank, checking or savings account\"\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Student loan\"\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Debt collection\"\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Consumer Loan\"\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Credit card or prepaid card\"\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Credit reporting\" # label cant have more than 50 characters\n",
        "          },\n",
        "          {\n",
        "              \"category\": \"Vehicle loan or lease\"\n",
        "          }\n",
        "      ],\n",
        "      \"documents\": [\n",
        "      ]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "SsVqk97g7kMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"/content/txt\"\n",
        "!mkdir \"/content/txt\"\n",
        "\n",
        "for index, row in tqdm(train.iterrows(), total=test.shape[0]):\n",
        "    with open(\"/content/txt/\" + str(row['Complaint ID']) + \".txt\", \"w\") as f:\n",
        "        f.write(row['Consumer complaint narrative'])\n",
        "    label = row['Product']\n",
        "    if label == \"Credit reporting, credit repair services, or other personal consumer reports\":\n",
        "        label = \"Credit reporting\"\n",
        "    metadata = {\n",
        "      \"location\": str(row['Complaint ID']) + \".txt\",\n",
        "      \"language\": \"en-us\",\n",
        "      \"dataset\": \"train\",\n",
        "      \"class\": {\n",
        "          \"category\": label\n",
        "      }\n",
        "    }\n",
        "    json_file['assets']['documents'].append(metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP5bn7aD5oB4",
        "outputId": "0a041054-5ccf-4590-810d-dc8119451c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "80121it [00:16, 4833.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/json_file_train\", \"w\") as fp:\n",
        "    json.dump(json_file, fp)\n",
        "!zip -r \"/content/drive/MyDrive/Bachelor Thesis/Datasets/txt_files_train.zip\" \"/content/txt\""
      ],
      "metadata": {
        "id": "Kb8_CgSibwOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continue with text preprocessing"
      ],
      "metadata": {
        "id": "3XsPiZWLVCSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_X(text):\n",
        "    x_free = text.replace('X', '')\n",
        "    x_free = re.sub(r' {2,}|\\t+', ' ', x_free)\n",
        "    return x_free\n",
        "\n",
        "train['Consumer complaint narrative'] = train['Consumer complaint narrative'].apply(lambda x:remove_X(x))\n",
        "test['Consumer complaint narrative'] = test['Consumer complaint narrative'].apply(lambda x:remove_X(x))\n",
        "train['Consumer complaint narrative'].values[18]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "e7wq7RIYk-e9",
        "outputId": "23ad875f-deed-4632-8418-88f67be5e4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I initially obtained a loan in for a car lease for a I obtained a loan from as of it was changed to I am in Predatory loan and need help out I am upside down in this loan from they were going to report me to the credit agency for 2700 yes dollars When you ask for extension they put so much interest on the deferred payment plan I dont think I will ever be done paying for this loan Please Help'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_digits(text):\n",
        "    digitfree = ''.join(i for i in text if not i.isdigit())\n",
        "    digitfree = re.sub(r' {2,}|\\t+', ' ', digitfree)\n",
        "    return digitfree\n",
        "\n",
        "train['Consumer complaint narrative'] = train['Consumer complaint narrative'].apply(lambda x:remove_digits(x))\n",
        "test['Consumer complaint narrative'] = test['Consumer complaint narrative'].apply(lambda x:remove_digits(x))\n",
        "train['Consumer complaint narrative'].values[18]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "6uGr8EVeeGt_",
        "outputId": "f50fe8da-7ace-4ca1-99d9-e364ee65f915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I initially obtained a loan in for a car lease for a I obtained a loan from as of it was changed to I am in Predatory loan and need help out I am upside down in this loan from they were going to report me to the credit agency for yes dollars When you ask for extension they put so much interest on the deferred payment plan I dont think I will ever be done paying for this loan Please Help'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['Consumer complaint narrative']= train['Consumer complaint narrative'].apply(lambda x: x.lower())\n",
        "test['Consumer complaint narrative']= test['Consumer complaint narrative'].apply(lambda x: x.lower())\n",
        "train['Consumer complaint narrative'].values[18]"
      ],
      "metadata": {
        "id": "Fxs9Pj4SA2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ee4afbf6-0be0-4291-affc-e85f8f8c9579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i initially obtained a loan in for a car lease for a i obtained a loan from as of it was changed to i am in predatory loan and need help out i am upside down in this loan from they were going to report me to the credit agency for yes dollars when you ask for extension they put so much interest on the deferred payment plan i dont think i will ever be done paying for this loan please help'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function for tokenization\n",
        "def tokenization(text):\n",
        "    tokens = re.split(' ', text)\n",
        "    tokens = [token for token in tokens if token]\n",
        "    return tokens\n",
        "#applying function to the column\n",
        "train['Consumer complaint narrative']= train['Consumer complaint narrative'].apply(lambda x: tokenization(x))\n",
        "test['Consumer complaint narrative']= test['Consumer complaint narrative'].apply(lambda x: tokenization(x))\n",
        "print(', '.join(map(repr, train['Consumer complaint narrative'].values[18]))) # use this function to show items horizontally, rather than vertically"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NKmlxNQA2ey",
        "outputId": "3f9af4ed-bc16-4203-b602-2dbd6460fa79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'i', 'initially', 'obtained', 'a', 'loan', 'in', 'for', 'a', 'car', 'lease', 'for', 'a', 'i', 'obtained', 'a', 'loan', 'from', 'as', 'of', 'it', 'was', 'changed', 'to', 'i', 'am', 'in', 'predatory', 'loan', 'and', 'need', 'help', 'out', 'i', 'am', 'upside', 'down', 'in', 'this', 'loan', 'from', 'they', 'were', 'going', 'to', 'report', 'me', 'to', 'the', 'credit', 'agency', 'for', 'yes', 'dollars', 'when', 'you', 'ask', 'for', 'extension', 'they', 'put', 'so', 'much', 'interest', 'on', 'the', 'deferred', 'payment', 'plan', 'i', 'dont', 'think', 'i', 'will', 'ever', 'be', 'done', 'paying', 'for', 'this', 'loan', 'please', 'help'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop words present in the library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "#defining the function to remove stopwords from tokenized text\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "#applying the function\n",
        "train['Consumer complaint narrative']= train['Consumer complaint narrative'].apply(lambda x:remove_stopwords(x))\n",
        "test['Consumer complaint narrative']= test['Consumer complaint narrative'].apply(lambda x:remove_stopwords(x))\n",
        "print(', '.join(map(repr, train['Consumer complaint narrative'].values[18])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCaY_dlBDK4b",
        "outputId": "ac2d1a72-74a4-4925-9031-dde248d5f49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'initially', 'obtained', 'loan', 'car', 'lease', 'obtained', 'loan', 'changed', 'predatory', 'loan', 'need', 'help', 'upside', 'loan', 'going', 'report', 'credit', 'agency', 'yes', 'dollars', 'ask', 'extension', 'put', 'much', 'interest', 'deferred', 'payment', 'plan', 'dont', 'think', 'ever', 'done', 'paying', 'loan', 'please', 'help'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_nonascii(text):\n",
        "    ascii_list = []\n",
        "    for word in text:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        ascii_list.append(new_word)\n",
        "    return ascii_list\n",
        "\n",
        "train['Consumer complaint narrative']= train['Consumer complaint narrative'].apply(lambda x:remove_stopwords(x))\n",
        "test['Consumer complaint narrative']= test['Consumer complaint narrative'].apply(lambda x:remove_stopwords(x))\n",
        "print(', '.join(map(repr, train['Consumer complaint narrative'].values[18])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTalRFTEMdq7",
        "outputId": "67037ff9-bbc4-4d7e-ae3c-edf8ac341cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'initially', 'obtained', 'loan', 'car', 'lease', 'obtained', 'loan', 'changed', 'predatory', 'loan', 'need', 'help', 'upside', 'loan', 'going', 'report', 'credit', 'agency', 'yes', 'dollars', 'ask', 'extension', 'put', 'much', 'interest', 'deferred', 'payment', 'plan', 'dont', 'think', 'ever', 'done', 'paying', 'loan', 'please', 'help'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['Consumer complaint narrative']= train['Consumer complaint narrative'].apply(lambda x: \" \".join(x))\n",
        "test['Consumer complaint narrative']= test['Consumer complaint narrative'].apply(lambda x: \" \".join(x))\n",
        "train['Consumer complaint narrative'].values[18]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "vxfwwUTp9Es7",
        "outputId": "2cf71ff2-5655-44c1-8765-3a97cbe90c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'initially obtained loan car lease obtained loan changed predatory loan need help upside loan going report credit agency yes dollars ask extension put much interest deferred payment plan dont think ever done paying loan please help'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "total_tokens = 0\n",
        "total_chars = 0\n",
        "all_tokens = []\n",
        "\n",
        "all_df = train.append(test)\n",
        "# iterate through each document in the \"Consumer complaint narrative\" column\n",
        "for i, row in tqdm(all_df.iterrows(), total=all_df.shape[0]):\n",
        "    # tokenize the document\n",
        "    tokens = tokenizer.encode(row[\"Consumer complaint narrative\"], max_length=2048, truncation=True)\n",
        "    # add tokens to list\n",
        "    all_tokens.extend(tokens)\n",
        "    # add the number of tokens to the total\n",
        "    total_tokens += len(tokens)\n",
        "    # add the number of characters to the total\n",
        "    total_chars += len(row[\"Consumer complaint narrative\"])\n",
        "\n",
        "# calculate the average number of tokens and characters per document\n",
        "avg_tokens = total_tokens / len(all_df)\n",
        "avg_chars = total_chars / len(all_df)\n",
        "\n",
        "print(\"\\nAverage tokens per document:\", avg_tokens)\n",
        "print(\"Average characters per document:\", avg_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1a4713-1ad9-4b21-a497-a4bcdcf2266d",
        "id": "hwcxShHVpS6q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94260/94260 [02:33<00:00, 612.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average tokens per document: 106.96298535964354\n",
            "Average characters per document: 707.9141311266709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of unique tokens in dataset:\", len(set(all_tokens)))\n",
        "print(\"Reduced dimensionality by \" + str(round(((1-len(set(all_tokens))/unique_tokens))*100,2)) + \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNb4JNapqdZf",
        "outputId": "60eeadfa-fd59-444f-a936-332f266d0491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens in dataset: 23486\n",
            "Reduced dimensionality by 30.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/train.csv\", sep='\\t', index=False)\n",
        "test.to_csv(\"/content/drive/MyDrive/Bachelor Thesis/Datasets/test.csv\", sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "hWx5h53_GE_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}